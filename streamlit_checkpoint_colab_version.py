# -*- coding: utf-8 -*-
"""Streamlit checkpoint 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ey5HvS8kW2TAeoZYaxHyS9uBlfmaSsEP
"""

import pandas as pd

df = pd.read_csv('/content/Expresso_churn_dataset.csv')

df.head()

df.info()

df.isnull().sum()

missing_percentage = df.isnull().mean() * 100
missing_percentage_sorted = missing_percentage.sort_values(ascending=False)
missing_percentage_sorted

df.drop(columns=['user_id','ZONE2', 'ZONE1', 'TIGO'], inplace=True)

df.describe()

df.shape

!pip install ydata-profiling
from ydata_profiling import ProfileReport

profile = ProfileReport(df, title="Expresso Churn Report", explorative=True)

profile.to_file("expresso_churn_report.html")

profile.to_notebook_iframe()

categorical_cols = df.select_dtypes(include=['object']).columns
categorical_cols

unique_namesG = df['TOP_PACK'].unique()
print(unique_namesG)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['REGION_encoded'] = le.fit_transform(df['REGION'])
le1 = LabelEncoder()
df['TENURE_encoded'] = le1.fit_transform(df['TENURE'])
le2 = LabelEncoder()
df['MRG_encoded'] = le2.fit_transform(df['MRG'])
le3 = LabelEncoder()
df['TOP_PACK_encoded'] = le3.fit_transform(df['TOP_PACK'])
le4 = LabelEncoder()
df['ARPU_SEGMENT_e'] = le4.fit_transform(df['ARPU_SEGMENT'])

df = df.fillna(0)

df1 = df.drop(['ARPU_SEGMENT'], axis=1)

# Remove duplicates, if any
df1.drop_duplicates(inplace=True)

df1.info()

df2= df1.drop(columns=['TOP_PACK','MRG', 'TENURE', 'REGION'])

df2.info()

df2.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
sns.boxplot(data=df2)
plt.xticks(rotation=90)
plt.title('Boxplots of the data')
plt.show()

Q1 = df2.quantile(0.25)
Q3 = df2.quantile(0.75)
IQR = Q3 - Q1

# Filter out outliers
df2 = df2[~((df2 < (Q1 - 1.5 * IQR)) | (df2 > (Q3 + 1.5 * IQR))).any(axis=1)]

df2.columns

len(df2.columns)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# List of columns to plot
columns = [
    'MONTANT', 'FREQUENCE_RECH', 'REVENUE', 'FREQUENCE', 'DATA_VOLUME',
       'ON_NET', 'ORANGE', 'REGULARITY', 'FREQ_TOP_PACK', 'CHURN',
       'REGION_encoded', 'TENURE_encoded', 'MRG_encoded', 'TOP_PACK_encoded',
       'ARPU_SEGMENT_e']

# Set up a plotting grid
n_cols = 3
n_rows = (len(columns) + n_cols - 1) // n_cols
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))

# Flatten axes for easy iteration
axes = axes.flatten()

# Plotting loop for histograms and bar plots
for idx, col in enumerate(columns):
    ax = axes[idx]
    if df2[col].dtype in ['int64', 'float64']:
        # If numeric, plot histogram
        ax.hist(df[col].dropna(), bins=20, color='orange', alpha=0.7)
        ax.set_title(f'Histogram of {col}')
        ax.set_xlabel(col)
        ax.set_ylabel('Frequency')
    else:
        # If categorical, plot bar plot
        value_counts = df[col].value_counts()
        ax.bar(value_counts.index.astype(str), value_counts.values, color='orange', alpha=0.7)
        ax.set_title(f'Bar Plot of {col}')
        ax.set_xlabel(col)
        ax.set_ylabel('Count')
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')

# Remove empty subplots if any
for j in range(idx + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

df2.describe()

from sklearn.preprocessing import StandardScaler

exclude_cols = ['TENURE_encoded',	'MRG_encoded',	'TOP_PACK_encoded',
                'REGION_encoded', 'CHURN', 'ARPU_SEGMENT_e']

numeric_cols = df2.select_dtypes(include=['float64', 'int64']).columns
cols_to_scale = [col for col in numeric_cols if col not in exclude_cols]

scaler = StandardScaler()
df1[cols_to_scale] = scaler.fit_transform(df1[cols_to_scale])

"""Based on the previous data exploration train and test a machine learning classifier"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Split the dataset into features (X) and target (y)
X = df2.drop('CHURN', axis=1)
y = df2['CHURN']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

import joblib
model = RandomForestClassifier()
# Save the trained model to a file
joblib.dump(model, 'expresso_churn_model.pkl')

import pickle

with open ('le.pkl', 'wb') as file:
  pickle.dump(le, file)

with open ('le1.pkl', 'wb') as file:
  pickle.dump(le1, file)

with open ('le2.pkl', 'wb') as file:
  pickle.dump(le2, file)

with open ('le3.pkl', 'wb') as file:
  pickle.dump(le3, file)

with open ('le4.pkl', 'wb') as file:
  pickle.dump(le4, file)

joblib.dump(scaler, 'scaler.pkl')

scaler = joblib.load('scaler.pkl')

df2.columns

df2.info()

unique_names7 = df['MRG'].unique()
print(unique_names7)

unique_names6 = df['TENURE'].unique()
print(unique_names6)

unique_names5 = df['REGION'].unique()
print(unique_names5)